import math
import os
from typing import Any, Callable, Dict, List, Literal, Optional

import torch
import torch.distributed as dist
import torch.nn as nn
import transformers
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy, StateDictType
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy, always_wrap_policy
from transformers import AutoModelForCausalLM, AutoConfig

from arealite.api.cli_args import EngineConfig, MicroBatchSpec, TrainingArgs
from arealite.api.engine_api import SPMDWrapper
from arealite.utils import split_dict_tensor_with_cu_seqlens
from realhf.api.cli_args import ParallelismConfig
from realhf.base.pkg_version import is_version_greater_or_equal

if is_version_greater_or_equal("torch", "2.6.0"):
    from torch.distributed.fsdp import CPUOffloadPolicy, FSDPModule, MixedPrecisionPolicy, fully_shard
elif is_version_greater_or_equal("torch", "2.4.0"):
    from torch.distributed._composable.fsdp import CPUOffloadPolicy, FSDPModule, MixedPrecisionPolicy, fully_shard
else:
    fully_shard, MixedPrecisionPolicy, FSDPModule, CPUOffloadPolicy = None, None, None, None

from torch.distributed.device_mesh import DeviceMesh, init_device_mesh


def get_transformer_layer_cls(model):
    """Get transformer layer classes for wrapping policy."""
    # Common transformer layer class names
    common_layer_names = ["Block", "DecoderLayer"]

    layer_classes = set()
    for name, module in model.named_modules():
        module_name = type(module).__name__
        if any(layer_name in module_name for layer_name in common_layer_names):
            layer_classes.add(type(module))

    # Fallback to standard PyTorch layers if none found
    if not layer_classes:
        layer_classes = {nn.TransformerEncoderLayer, nn.TransformerDecoderLayer}

    return layer_classes

def create_fsdp_device_mesh(shard_size, world_size):
    if shard_size < 0 or shard_size >= world_size:
        device_mesh = init_device_mesh(
            "cuda", mesh_shape=(world_size,), mesh_dim_names=("fsdp",)
        )
    else:
        device_mesh = init_device_mesh(
            "cuda",
            mesh_shape=(world_size // shard_size, shard_size),
            mesh_dim_names=("ddp", "fsdp"),
        )
    return device_mesh

def apply_fsdp2(model, fsdp_kwargs):
    """model: AutoModelForCausalLM"""
    assert CPUOffloadPolicy is not None, "PyTorch version >= 2.4 is required for using fully_shard API (FSDP2)"

    default_transformer_cls_names_to_wrap = getattr(model, "_no_split_modules", None)
    # fsdp_transformer_layer_cls_to_wrap = config.get("wrap_policy", {}).get("transformer_layer_cls_to_wrap", default_transformer_cls_names_to_wrap)
    fsdp_transformer_layer_cls_to_wrap = default_transformer_cls_names_to_wrap

    if isinstance(fsdp_transformer_layer_cls_to_wrap, str):
        fsdp_transformer_layer_cls_to_wrap = [fsdp_transformer_layer_cls_to_wrap]

    assert len(fsdp_transformer_layer_cls_to_wrap) > 0 and fsdp_transformer_layer_cls_to_wrap[0] is not None

    modules = []
    for name, module in model.named_modules():
        if (module.__class__.__name__ in fsdp_transformer_layer_cls_to_wrap 
            or (isinstance(module, nn.Embedding) and not model.config.tie_word_embeddings)):
            modules.append(module)

    for idx, module in enumerate(modules):
        fully_shard(module, **fsdp_kwargs)
    fully_shard(model, **fsdp_kwargs)  # fsdp2 will not reshard_after_forward for root module

def fsdp2_load_full_state_dict(model: torch.nn.Module, full_state: dict, device_mesh=None, cpu_offload=None):
    """
    Loads the full state dict (could be only on rank 0) into the sharded model. This is done by broadcasting the
    parameters from rank 0 to all other ranks. This function modifies the model in-place.

    Args:
        model (`torch.nn.Module`): The model to load the state dict into
        full_state (`dict`): The full state dict to load, can only be on rank 0
    """
    from torch.distributed.checkpoint.state_dict import StateDictOptions, set_model_state_dict
    device = torch.cuda.current_device()

    # To broadcast, it needs to be instantiated in the GPU.
    if dist.get_rank() == 0:
        model = model.to(device=device, non_blocking=True)
    else:
        model = model.to_empty(device=device)

    cpu_offload = cpu_offload is not None
    options = StateDictOptions(full_state_dict=True, cpu_offload=cpu_offload, broadcast_from_rank0=True)
    set_model_state_dict(model, full_state, options=options)

    # rotary_emb is not in state_dict, so we need to broadcast it manually
    for name, buf in model.named_buffers():
        dist.broadcast(buf, src=0)

    if cpu_offload:
        model.to("cpu", non_blocking=True)
        for buf in model.buffers():
            buf.data = buf.data.to(device)


def get_cosine_schedule_with_warmup(
    optimizer: torch.optim.Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    min_lr_ratio: float = 0.0,
    num_cycles: float = 0.5,
    last_epoch: int = -1,
):
    """
    Create a schedule with a learning rate that decreases following the values of the cosine function between the
    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
    initial lr set in the optimizer.
    Args:
        optimizer (:class:`~torch.optim.Optimizer`):
            The optimizer for which to schedule the learning rate.
        num_warmup_steps (:obj:`int`):
            The number of steps for the warmup phase.
        num_training_steps (:obj:`int`):
            The total number of training steps.
        min_lr_ratio (:obj:`float`, `optional`, defaults to 0.0):
            The minimum lr ratio w.r.t the maximum.
        num_cycles (:obj:`float`, `optional`, defaults to 0.5):
            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
            following a half-cosine).
        last_epoch (:obj:`int`, `optional`, defaults to -1):
            The index of the last epoch when resuming training.
    Return:
        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.
    """
    assert min_lr_ratio >= 0 and min_lr_ratio <= 1.0
    coef = (1 - min_lr_ratio) * 0.5
    intercept = (1 + min_lr_ratio) * 0.5

    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(
            max(1, num_training_steps - num_warmup_steps)
        )
        x = math.cos(math.pi * float(num_cycles) * 2.0 * progress)
        return max(0.0, x * coef + intercept)

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)


class FSDPEngine(SPMDWrapper):
    """Simplified FSDP engine for transformer models."""

    def __init__(self, args: TrainingArgs, engine_config: EngineConfig):
        super().__init__(args, engine_config)
        assert is_version_greater_or_equal("torch", "2.4.0"), f"arealite only supports FSDP2, which requires torch>=2.4.0"

        self.config = engine_config.backend.fsdp

        self.model = None
        self.optimizer = None
        self.model_config = None
        self.device_mesh = None
        self.cpu_offload = None

        self.world_size = args.n_gpus_per_node * args.n_nodes

    def init_distributed(self, config: ParallelismConfig):
        """Initialize distributed communication and model."""
        if not dist.is_initialized():
            dist.init_process_group(backend="nccl")

        # Load model
        dtype = torch.bfloat16 if self.engine_config.bf16 else torch.float16
        model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=self.engine_config.path,
            torch_dtype=dtype,
            attn_implementation="flash_attention_2",
            trust_remote_code=True,
        )
        self.model_config = AutoConfig.from_pretrained(
            pretrained_model_name_or_path=self.engine_config.path, 
            trust_remote_code=True
        )

        # Simple auto wrap policy
        # TODO: fix wrap policy
        auto_wrap_policy = always_wrap_policy

        mixed_precision_policy = MixedPrecisionPolicy(param_dtype=torch.bfloat16, reduce_dtype=torch.float32, cast_forward_inputs=True)
        device_mesh = create_fsdp_device_mesh(self.world_size, self.world_size) 
        self.device_mesh = device_mesh
        # sharding_strategy = ShardingStrategy.FULL_SHARD
        cpu_offload = None
        self.cpu_offload = cpu_offload

        fsdp_kwargs = {
            "mesh": device_mesh,
            "mp_policy": mixed_precision_policy,
            "offload_policy": cpu_offload,
            "reshard_after_forward": True,
        }

        # Wrap with FSDP2
        # self.model = FSDP(
        #     model,
        #     auto_wrap_policy=auto_wrap_policy,
        #     sharding_strategy=ShardingStrategy.FULL_SHARD,
        #     device_id=torch.cuda.current_device(),
        #     sync_module_states=self.config.sync_module_states,
        #     use_orig_params=self.config.use_orig_params,
        # )

        full_state = model.state_dict()
        apply_fsdp2(model, fsdp_kwargs)
        fsdp2_load_full_state_dict(model, full_state, device_mesh, cpu_offload)
        
        self.model = model

        # Set up optimizer
        optimizer_config = self.engine_config.optimizer
        if optimizer_config is not None:
            assert (
                optimizer_config.type == "adam"
            ), "Only AdamW optimizer is supported in this engine."
            lr = optimizer_config.lr
            weight_decay = optimizer_config.weight_decay
            beta1 = optimizer_config.beta1
            beta2 = optimizer_config.beta2
            eps = optimizer_config.eps

            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=lr,
                weight_decay=weight_decay,
                betas=(beta1, beta2),
                eps=eps,
            )
            # TODO: get total training steps
            total_train_steps = 1000
            num_warmup_steps = int(optimizer_config.warmup_steps_proportion * total_train_steps)

            self.lr_scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps,
                total_train_steps,
                min_lr_ratio=optimizer_config.min_lr_ratio,
            )

    # def _split_microbatches(self, input_: Dict, mb_spec: MicroBatchSpec) -> List[Dict]:
    #     """Split input into microbatches."""
    #     batch_size = len(input_["input_ids"])
    #     n_mbs = min(mb_spec.n_mbs, batch_size)
    #     mb_size = batch_size // n_mbs

    #     microbatches = []
    #     for i in range(n_mbs):
    #         start = i * mb_size
    #         end = start + mb_size if i < n_mbs - 1 else batch_size

    #         mb = {}
    #         for key, value in input_.items():
    #             if isinstance(value, (list, torch.Tensor)):
    #                 mb[key] = value[start:end]
    #             else:
    #                 mb[key] = value
    #         microbatches.append(mb)

    #     return microbatches

    # def _initialize_fsdp_train(self):
    #     if not self.train_initialized:
    #         for fsdp_state in traversal_utils._get_fsdp_states(self.module):
    #             fsdp_state._is_root = None
    #         self.train_initialized = True

    def train_batch(
        self,
        input_: Dict,
        mb_spec: MicroBatchSpec,
        loss_fn: Callable[[torch.Tensor, Dict], torch.Tensor],
        loss_weight_fn: Callable[[Dict], float],
        version_steps: int,
        token_normalize_scope: Literal["global", "dp"] = "global",
    ) -> Dict:
        """Train on a batch using gradient accumulation."""
        # self._initialize_fsdp_train()
        assert self.optimizer is not None
        assert self.lr_scheduler is not None

        self.model.train()
        self.optimizer.zero_grad()

        mb_inputs = split_dict_tensor_with_cu_seqlens(input_, mb_spec)

        total_loss_weight = torch.tensor(
            sum([loss_weight_fn(mb) for mb in mb_inputs]), dtype=torch.float32
        )
        assert total_loss_weight != 0
        if token_normalize_scope == "global":
            dist.all_reduce(total_loss_weight)

        # total_loss = 0.0
        # total_n_tokens = 0
        # Process microbatches with gradient accumulation
        for mb_input in mb_inputs:
            outputs = self.model(**mb_input)
            loss = loss_fn(outputs.logits, mb_input)
            loss_scale = loss_weight_fn(mb_input) / total_loss_weight

            # Scale loss for accumulation
            # TODO: check if this is required for FSDP
            if token_normalize_scope == "global":
                loss_scale *= self.world_size
            
            loss *= loss_scale
            loss.backward()

        # Optimizer step
        self.optimizer.step()

        return {}

    @torch.no_grad()
    def eval_batch(
        self,
        input_: Dict,
        mb_spec: MicroBatchSpec,
        loss_fn: Callable[[torch.Tensor, Dict], torch.Tensor],
    ) -> torch.Tensor | None:
        """Evaluate on a batch."""
        self.model.eval()

        assert "cu_seqlens" in input_
        lens = input_["cu_seqlens"]
        mb_inputs = split_dict_tensor_with_cu_seqlens(input_, mb_spec)

        total_loss = 0.0
        total_weight = 0.0

        for mb_input in mb_inputs:
            outputs = self.model(**mb_input)
            loss = loss_fn(outputs.logits, mb_input)

            # Simple weight calculation (could be improved)
            weight = mb_input["input_ids"].numel()

            total_loss += loss.item() * weight
            total_weight += weight

        return torch.tensor(total_loss / max(total_weight, 1e-8))

    @torch.no_grad()
    def forward(
        self,
        input_: Dict,
        mb_spec: MicroBatchSpec,
        output_seqlens: List[List[int]] | None = None,
        post_hook: Callable[[torch.Tensor, Dict], Any] | None = None,
        aggregate_fn: Callable[[List[Any]], Any] = torch.cat,
    ) -> Any | None:
        """Forward pass with optional post-processing."""
        self.model.eval()
        mb_inputs = split_dict_tensor_with_cu_seqlens(input_, mb_spec)
        
        results = []

        for mb_input in mb_inputs:
            outputs = self.model(**mb_input)

            if post_hook:
                result = post_hook(outputs.logits, mb_input)
                results.append(result)
            else:
                results.append(outputs.logits)

        return aggregate_fn(results) if results else None

    def get_hf_model_state_dict(self) -> Dict[str, torch.Tensor]:
        """Get model state dict for saving."""
        if self.model is None:
            raise RuntimeError("Model not initialized")

        with FSDP.state_dict_type(self.model, StateDictType.FULL_STATE_DICT):
            return self.model.state_dict()

    def save_model_to_hf(
        self,
        tokenizer: transformers.PreTrainedTokenizerFast,
        path: str,
        base_model_path: Optional[str] = None,
    ):
        """Save model in HuggingFace format."""
        if self.model is None:
            raise RuntimeError("Model not initialized")

        os.makedirs(path, exist_ok=True)

        # FSDP2 checkpoint saving
        from torch.distributed.checkpoint.state_dict import StateDictOptions, get_model_state_dict

        # Get full state dict with FSDP2
        options = StateDictOptions(full_state_dict=True, cpu_offload=True)
        state_dict = get_model_state_dict(self.model, options=options)

        # save huggingface model
        if dist.get_rank() == 0:
            os.makedirs(path, exist_ok=True)
            self.model.save_pretrained(path, state_dict=state_dict)
            self.model_config.save_pretrained(path)
            tokenizer.save_pretrained(path)

        dist.barrier()


    def load_model_from_hf(self, path: str):
        """Load model from HuggingFace format."""
        # if self.model is None:
        #     raise RuntimeError("Model not initialized")

        # state_dict = torch.load(
        #     os.path.join(path, "pytorch_model.bin"), map_location="cpu"
        # )

        # with FSDP.state_dict_type(self.model, StateDictType.FULL_STATE_DICT):
        #     self.model.load_state_dict(state_dictt
        dtype = torch.bfloat16 if self.engine_config.bf16 else torch.float16
        model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=path,
            torch_dtype=dtype,
            attn_implementation="flash_attention_2",
            trust_remote_code=True,
        )
        full_state = model.state_dict()

        fsdp2_load_full_state_dict(self.model, full_state, self.device_mesh, self.cpu_offload)


    def save_optimizer_state(self, path: str):
        """Save optimizer state."""
        if self.optimizer is None:
            raise RuntimeError("Optimizer not initialized")

        os.makedirs(path, exist_ok=True)
        torch.save(self.optimizer.state_dict(), os.path.join(path, "optimizer.pt"))

    def load_optimizer_state(self, path: str):
        """Load optimizer state."""
        if self.optimizer is None:
            raise RuntimeError("Optimizer not initialized")

        optimizer_path = os.path.join(path, "optimizer.pt")
        if os.path.exists(optimizer_path):
            self.optimizer.load_state_dict(
                torch.load(optimizer_path, map_location="cpu")
            )
